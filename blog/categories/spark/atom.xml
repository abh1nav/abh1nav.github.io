<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: spark | sqldump]]></title>
  <link href="http://blog.abhinav.ca/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://blog.abhinav.ca/"/>
  <updated>2014-07-31T11:43:04-04:00</updated>
  <id>http://blog.abhinav.ca/</id>
  <author>
    <name><![CDATA[Abhinav Ajgaonkar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setup a Spark Cluster in 5 minutes]]></title>
    <link href="http://blog.abhinav.ca/blog/2014/04/13/setup-a-spark-cluster-in-5-minutes/"/>
    <updated>2014-04-13T23:31:45-04:00</updated>
    <id>http://blog.abhinav.ca/blog/2014/04/13/setup-a-spark-cluster-in-5-minutes</id>
    <content type="html"><![CDATA[<h4>Prerequisites</h4>

<p>Assuming you have 3 nodes: <code>node1, node2, node3</code>, ensure the hosts file contains the following entries on all nodes:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.1.100  node1
</span><span class='line'>192.168.1.101  node2
</span><span class='line'>192.168.1.102  node3
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Be sure replace <code>192.168.x.x</code> with the actual IPs for each node.</p>

<h4>Get Spark</h4>

<p>Download binaries from <a href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a> and extract it to <code>/opt/</code>.</p>

<h4>Spark Master</h4>

<p>Start the master process on <code>node1</code></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd</span> /opt/spark-0.9.0-incubating-bin-hadoop1
</span><span class='line'>sbin/start-master.sh
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h4>Spark Workers</h4>

<p>Start worker processes on each node:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd</span> /opt/spark-0.9.0-incubating-bin-hadoop1
</span><span class='line'>bin/spark-class org.apache.spark.deploy.worker.Worker spark://node1:7077
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Check the Spark UI at <code>http://node1:8080</code> to make sure all workers have registered with the master.</p>
]]></content>
  </entry>
  
</feed>
